{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E5: Build Heterogeneous Graph (HeteroData)\n",
    "\n",
    "**Milestone:** E5 - Heterogeneous Graph Construction  \n",
    "**Objective:** Build PyG HeteroData from Elliptic++ CSV files with temporal constraints\n",
    "\n",
    "**Node Types:**\n",
    "- Transaction: 203,769 nodes (93 local features)\n",
    "- Address: 823,942 nodes (52 features)\n",
    "\n",
    "**Edge Types:**\n",
    "- tx → tx: Transaction flows\n",
    "- addr → tx: Address inputs to transaction\n",
    "- tx → addr: Transaction outputs to address\n",
    "- addr → addr: Address-to-address connections\n",
    "\n",
    "**Output:**\n",
    "- `hetero_graph.pt`: PyG HeteroData object\n",
    "- `hetero_graph_summary.json`: Graph statistics\n",
    "- `node_mappings.json`: ID mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torch-geometric pandas numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import json\n",
    "from torch_geometric.data import HeteroData\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_ROOT = Path('/kaggle/input/elliptic2')  # Adjust to your Kaggle dataset path\n",
    "OUTPUT_DIR = Path('/kaggle/working')\n",
    "\n",
    "# Graph construction settings\n",
    "TOP_K_ADDRESSES = 100000  # Use top 100K most active addresses (MVP)\n",
    "USE_ALL_ADDRESSES = False  # Set True to use all 823K addresses (requires ~12GB RAM)\n",
    "\n",
    "# Temporal splits (same as E3)\n",
    "TRAIN_FRAC = 0.6\n",
    "VAL_FRAC = 0.2\n",
    "TEST_FRAC = 0.2\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR}\")\n",
    "print(f\"Address strategy: {'All addresses' if USE_ALL_ADDRESSES else f'Top {TOP_K_ADDRESSES:,}'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Transaction Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading transaction nodes...\")\n",
    "\n",
    "# Load features and classes\n",
    "tx_features = pd.read_csv(DATA_ROOT / 'txs_features.csv')\n",
    "tx_classes = pd.read_csv(DATA_ROOT / 'txs_classes.csv')\n",
    "\n",
    "# Merge\n",
    "tx_data = tx_features.merge(tx_classes, on='txId', how='left')\n",
    "\n",
    "print(f\"  Transactions loaded: {len(tx_data):,}\")\n",
    "\n",
    "# Create ID mappings\n",
    "tx_ids = tx_data['txId'].values\n",
    "tx_id_to_idx = {tx_id: idx for idx, tx_id in enumerate(tx_ids)}\n",
    "tx_idx_to_id = {idx: tx_id for idx, tx_id in enumerate(tx_ids)}\n",
    "\n",
    "# Extract LOCAL features only (AF1-AF93)\n",
    "local_features = [col for col in tx_data.columns if 'Local_feature' in col]\n",
    "print(f\"  Features: {len(local_features)} (Local only)\")\n",
    "\n",
    "# Extract features\n",
    "tx_x = torch.FloatTensor(tx_data[local_features].values)\n",
    "tx_x = torch.nan_to_num(tx_x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# Normalize\n",
    "tx_x = (tx_x - tx_x.mean(dim=0)) / (tx_x.std(dim=0) + 1e-8)\n",
    "tx_x = torch.nan_to_num(tx_x, nan=0.0)\n",
    "\n",
    "# Extract timestamps\n",
    "tx_timestamps = torch.LongTensor(tx_data['Time step'].values)\n",
    "\n",
    "# Extract labels (1=illicit→1, 2=licit→0, 3/NaN=unknown→-1)\n",
    "y_raw = tx_data['class'].fillna(3).astype(int).values\n",
    "tx_y = torch.LongTensor(np.where(y_raw == 1, 1, np.where(y_raw == 2, 0, -1)))\n",
    "\n",
    "print(f\"  Labeled: {(tx_y >= 0).sum():,} / {len(tx_y):,}\")\n",
    "print(f\"  Fraud: {(tx_y == 1).sum():,}, Legit: {(tx_y == 0).sum():,}\")\n",
    "print(f\"  Feature shape: {tx_x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Address Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading address nodes...\")\n",
    "\n",
    "# Try combined file first (more efficient)\n",
    "combined_file = DATA_ROOT / 'wallets_features_classes_combined.csv'\n",
    "\n",
    "if combined_file.exists():\n",
    "    print(\"  Using combined wallet file...\")\n",
    "    addr_data = pd.read_csv(combined_file)\n",
    "else:\n",
    "    print(\"  Loading separate files...\")\n",
    "    addr_features = pd.read_csv(DATA_ROOT / 'wallets_features.csv')\n",
    "    addr_classes = pd.read_csv(DATA_ROOT / 'wallets_classes.csv')\n",
    "    addr_data = addr_features.merge(addr_classes, on='address', how='left')\n",
    "\n",
    "print(f\"  Total addresses: {len(addr_data):,}\")\n",
    "\n",
    "# Select top K most active if not using all\n",
    "if not USE_ALL_ADDRESSES and TOP_K_ADDRESSES:\n",
    "    addr_data = addr_data.nlargest(TOP_K_ADDRESSES, 'total_txs')\n",
    "    print(f\"  Selected top {TOP_K_ADDRESSES:,} most active addresses\")\n",
    "\n",
    "# Create ID mappings\n",
    "addr_ids = addr_data['address'].values\n",
    "addr_id_to_idx = {addr_id: idx for idx, addr_id in enumerate(addr_ids)}\n",
    "addr_idx_to_id = {idx: addr_id for idx, addr_id in enumerate(addr_ids)}\n",
    "\n",
    "# Extract features (exclude ID, timestamp, class)\n",
    "feature_cols = [col for col in addr_data.columns \n",
    "                if col not in ['address', 'Time step', 'class']]\n",
    "print(f\"  Features: {len(feature_cols)}\")\n",
    "\n",
    "# Extract features\n",
    "addr_x = torch.FloatTensor(addr_data[feature_cols].values)\n",
    "addr_x = torch.nan_to_num(addr_x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# Normalize\n",
    "addr_x = (addr_x - addr_x.mean(dim=0)) / (addr_x.std(dim=0) + 1e-8)\n",
    "addr_x = torch.nan_to_num(addr_x, nan=0.0)\n",
    "\n",
    "# Extract timestamps\n",
    "addr_timestamps = torch.LongTensor(addr_data['Time step'].values)\n",
    "\n",
    "# Extract labels\n",
    "y_raw = addr_data['class'].fillna(3).astype(int).values\n",
    "addr_y = torch.LongTensor(np.where(y_raw == 1, 1, np.where(y_raw == 2, 0, -1)))\n",
    "\n",
    "print(f\"  Labeled: {(addr_y >= 0).sum():,} / {len(addr_y):,}\")\n",
    "print(f\"  Fraud: {(addr_y == 1).sum():,}, Legit: {(addr_y == 0).sum():,}\")\n",
    "print(f\"  Feature shape: {addr_x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_edges(edge_type, src_mapping, dst_mapping):\n",
    "    \"\"\"\n",
    "    Load edges for a specific type.\n",
    "    \n",
    "    Args:\n",
    "        edge_type: One of ['tx-tx', 'addr-tx', 'tx-addr', 'addr-addr']\n",
    "        src_mapping: Source node ID to index mapping\n",
    "        dst_mapping: Destination node ID to index mapping\n",
    "    \n",
    "    Returns:\n",
    "        edge_index: [2, E] tensor\n",
    "    \"\"\"\n",
    "    file_map = {\n",
    "        'tx-tx': 'txs_edgelist.csv',\n",
    "        'addr-tx': 'AddrTx_edgelist.csv',\n",
    "        'tx-addr': 'TxAddr_edgelist.csv',\n",
    "        'addr-addr': 'AddrAddr_edgelist.csv'\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nLoading {edge_type} edges...\")\n",
    "    \n",
    "    edges_df = pd.read_csv(DATA_ROOT / file_map[edge_type])\n",
    "    cols = list(edges_df.columns)\n",
    "    src_col, dst_col = cols[0], cols[1]\n",
    "    \n",
    "    print(f\"  Total edges in file: {len(edges_df):,}\")\n",
    "    \n",
    "    # Filter to valid nodes\n",
    "    valid = (edges_df[src_col].isin(src_mapping) & \n",
    "             edges_df[dst_col].isin(dst_mapping))\n",
    "    \n",
    "    src_idx = edges_df.loc[valid, src_col].map(src_mapping).values\n",
    "    dst_idx = edges_df.loc[valid, dst_col].map(dst_mapping).values\n",
    "    \n",
    "    edge_index = torch.LongTensor(np.vstack([src_idx, dst_idx]))\n",
    "    \n",
    "    print(f\"  Valid edges: {edge_index.shape[1]:,}\")\n",
    "    \n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all edge types\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING EDGES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "edge_index_tx_tx = load_edges('tx-tx', tx_id_to_idx, tx_id_to_idx)\n",
    "edge_index_addr_tx = load_edges('addr-tx', addr_id_to_idx, tx_id_to_idx)\n",
    "edge_index_tx_addr = load_edges('tx-addr', tx_id_to_idx, addr_id_to_idx)\n",
    "\n",
    "# addr-addr edges may be very large - optional\n",
    "try:\n",
    "    edge_index_addr_addr = load_edges('addr-addr', addr_id_to_idx, addr_id_to_idx)\n",
    "except Exception as e:\n",
    "    print(f\"\\nSkipping addr-addr edges: {e}\")\n",
    "    edge_index_addr_addr = torch.LongTensor([[],[]])  # Empty edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Temporal Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreating temporal splits...\")\n",
    "\n",
    "# Sort timestamps and find boundaries\n",
    "sorted_times = np.sort(np.unique(tx_timestamps.numpy()))\n",
    "n_timesteps = len(sorted_times)\n",
    "\n",
    "train_end_idx = int(n_timesteps * TRAIN_FRAC)\n",
    "val_end_idx = int(n_timesteps * (TRAIN_FRAC + VAL_FRAC))\n",
    "\n",
    "train_time_end = sorted_times[train_end_idx - 1]\n",
    "val_time_end = sorted_times[val_end_idx - 1]\n",
    "\n",
    "# Create masks (only for labeled transactions)\n",
    "labeled = tx_y >= 0\n",
    "train_mask = (tx_timestamps <= train_time_end) & labeled\n",
    "val_mask = ((tx_timestamps > train_time_end) & (tx_timestamps <= val_time_end)) & labeled\n",
    "test_mask = (tx_timestamps > val_time_end) & labeled\n",
    "\n",
    "print(f\"  Train: {train_mask.sum():,} (time <= {train_time_end})\")\n",
    "print(f\"  Val:   {val_mask.sum():,} (time <= {val_time_end})\")\n",
    "print(f\"  Test:  {test_mask.sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build HeteroData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BUILDING HETERODATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize HeteroData\n",
    "data = HeteroData()\n",
    "\n",
    "# Add transaction nodes\n",
    "data['transaction'].x = tx_x\n",
    "data['transaction'].y = tx_y\n",
    "data['transaction'].timestamp = tx_timestamps\n",
    "data['transaction'].train_mask = train_mask\n",
    "data['transaction'].val_mask = val_mask\n",
    "data['transaction'].test_mask = test_mask\n",
    "\n",
    "# Add address nodes\n",
    "data['address'].x = addr_x\n",
    "data['address'].y = addr_y\n",
    "data['address'].timestamp = addr_timestamps\n",
    "\n",
    "# Add edges\n",
    "data['transaction', 'to', 'transaction'].edge_index = edge_index_tx_tx\n",
    "data['address', 'to', 'transaction'].edge_index = edge_index_addr_tx\n",
    "data['transaction', 'to', 'address'].edge_index = edge_index_tx_addr\n",
    "data['address', 'to', 'address'].edge_index = edge_index_addr_addr\n",
    "\n",
    "print(\"\\nHeteroData Summary:\")\n",
    "print(data)\n",
    "\n",
    "print(\"\\nNode Statistics:\")\n",
    "print(f\"  Transactions: {data['transaction'].num_nodes:,}\")\n",
    "print(f\"  Addresses: {data['address'].num_nodes:,}\")\n",
    "\n",
    "print(\"\\nEdge Statistics:\")\n",
    "for edge_type in data.edge_types:\n",
    "    src, rel, dst = edge_type\n",
    "    print(f\"  {src} -> {dst}: {data[edge_type].num_edges:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING OUTPUTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save HeteroData\n",
    "torch.save(data, OUTPUT_DIR / 'hetero_graph.pt')\n",
    "print(f\"\\nSaved HeteroData: {OUTPUT_DIR / 'hetero_graph.pt'}\")\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'num_nodes': {\n",
    "        'transaction': data['transaction'].num_nodes,\n",
    "        'address': data['address'].num_nodes\n",
    "    },\n",
    "    'num_edges': {\n",
    "        f\"{src}_to_{dst}\": data[src, rel, dst].num_edges\n",
    "        for src, rel, dst in data.edge_types\n",
    "    },\n",
    "    'num_labeled': {\n",
    "        'transaction': (data['transaction'].y >= 0).sum().item(),\n",
    "        'address': (data['address'].y >= 0).sum().item()\n",
    "    },\n",
    "    'temporal_range': {\n",
    "        'transaction': [\n",
    "            data['transaction'].timestamp.min().item(),\n",
    "            data['transaction'].timestamp.max().item()\n",
    "        ],\n",
    "        'address': [\n",
    "            data['address'].timestamp.min().item(),\n",
    "            data['address'].timestamp.max().item()\n",
    "        ]\n",
    "    },\n",
    "    'feature_dims': {\n",
    "        'transaction': data['transaction'].x.shape[1],\n",
    "        'address': data['address'].x.shape[1]\n",
    "    },\n",
    "    'splits': {\n",
    "        'train': data['transaction'].train_mask.sum().item(),\n",
    "        'val': data['transaction'].val_mask.sum().item(),\n",
    "        'test': data['transaction'].test_mask.sum().item()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'hetero_graph_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"Saved summary: {OUTPUT_DIR / 'hetero_graph_summary.json'}\")\n",
    "\n",
    "# Save node mappings (first 1000 of each for reference)\n",
    "mappings = {\n",
    "    'tx_id_to_idx_sample': {str(k): int(v) for k, v in list(tx_id_to_idx.items())[:1000]},\n",
    "    'addr_id_to_idx_sample': {str(k): int(v) for k, v in list(addr_id_to_idx.items())[:1000]}\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'node_mappings_sample.json', 'w') as f:\n",
    "    json.dump(mappings, f, indent=2)\n",
    "print(f\"Saved mappings: {OUTPUT_DIR / 'node_mappings_sample.json'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"E5 MILESTONE COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nNext: Download hetero_graph.pt for E6 (TRD-HHGTN training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VALIDATION CHECKS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check 1: Node counts match\n",
    "print(\"\\n1. Node Counts:\")\n",
    "print(f\"   Transactions: {data['transaction'].num_nodes:,} (expected: {len(tx_ids):,})\")\n",
    "print(f\"   Addresses: {data['address'].num_nodes:,} (expected: {len(addr_ids):,})\")\n",
    "assert data['transaction'].num_nodes == len(tx_ids), \"Transaction count mismatch!\"\n",
    "assert data['address'].num_nodes == len(addr_ids), \"Address count mismatch!\"\n",
    "print(\"   ✓ PASS\")\n",
    "\n",
    "# Check 2: Edge indices are valid\n",
    "print(\"\\n2. Edge Index Validity:\")\n",
    "for edge_type in data.edge_types:\n",
    "    src, rel, dst = edge_type\n",
    "    edge_index = data[edge_type].edge_index\n",
    "    if edge_index.shape[1] > 0:  # Only check non-empty edges\n",
    "        src_max = edge_index[0].max().item()\n",
    "        dst_max = edge_index[1].max().item()\n",
    "        src_nodes = data[src].num_nodes\n",
    "        dst_nodes = data[dst].num_nodes\n",
    "        print(f\"   {src} -> {dst}: src_max={src_max} < {src_nodes}, dst_max={dst_max} < {dst_nodes}\")\n",
    "        assert src_max < src_nodes, f\"{src} index out of bounds!\"\n",
    "        assert dst_max < dst_nodes, f\"{dst} index out of bounds!\"\n",
    "print(\"   ✓ PASS\")\n",
    "\n",
    "# Check 3: Split sizes\n",
    "print(\"\\n3. Split Integrity:\")\n",
    "train_count = data['transaction'].train_mask.sum().item()\n",
    "val_count = data['transaction'].val_mask.sum().item()\n",
    "test_count = data['transaction'].test_mask.sum().item()\n",
    "total_labeled = (data['transaction'].y >= 0).sum().item()\n",
    "print(f\"   Train: {train_count:,}\")\n",
    "print(f\"   Val: {val_count:,}\")\n",
    "print(f\"   Test: {test_count:,}\")\n",
    "print(f\"   Total: {train_count + val_count + test_count:,} (labeled: {total_labeled:,})\")\n",
    "assert train_count + val_count + test_count == total_labeled, \"Split count mismatch!\"\n",
    "print(\"   ✓ PASS\")\n",
    "\n",
    "# Check 4: No NaN in features\n",
    "print(\"\\n4. Feature Quality:\")\n",
    "tx_nans = torch.isnan(data['transaction'].x).sum().item()\n",
    "addr_nans = torch.isnan(data['address'].x).sum().item()\n",
    "print(f\"   Transaction NaNs: {tx_nans}\")\n",
    "print(f\"   Address NaNs: {addr_nans}\")\n",
    "assert tx_nans == 0, \"Transaction features contain NaN!\"\n",
    "assert addr_nans == 0, \"Address features contain NaN!\"\n",
    "print(\"   ✓ PASS\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL VALIDATION CHECKS PASSED!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
